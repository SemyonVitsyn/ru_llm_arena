{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from gen_answer import AnswerGenerator\n",
    "from gen_judgment import Judge\n",
    "from show_result import Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_JUDGE = None                # judge model from API                      (\"gpt-4-1106-preview\")\n",
    "LLM_MODEL = None                # testing model from API                    (\"gemma-2-9b-it\")\n",
    "\n",
    "GEN_ANSWER_CONFIG_NAME = None   # necessary if setup is not default         (\"temp-0.2_tokens-4k\")\n",
    "OVERRIDE_GEN_PARAMS = None      # optional if such setup already created    ({temperature: 0.2, max_tokens: 4096})\n",
    "\n",
    "JUDGE_CONFIG_NAME = None        # necessary if setup is not default         (\"without_pairwise\")\n",
    "OVERRIDE_JUDGE_PARAMS = None    # optional if such setup already created    ({pairwise: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_gen_config_path = \"default_config/gen_answer_config.yaml\"\n",
    "default_judge_config_path = \"default_config/judge_config.yaml\"\n",
    "api_config_path = \"default_config/api_config.yaml\"\n",
    "data_path = \"data\"\n",
    "\n",
    "\n",
    "with open(default_gen_config_path, 'r') as f:\n",
    "    gen_config = yaml.safe_load(f)\n",
    "\n",
    "with open(default_judge_config_path, 'r') as f:\n",
    "    judge_config = yaml.safe_load(f)\n",
    "\n",
    "with open(api_config_path, 'r') as f:\n",
    "    api_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_configs(dict1, dict2):\n",
    "    dict1_filtered = {k: v for k, v in dict1.items() if k != \"model_list\"}\n",
    "    dict2_filtered = {k: v for k, v in dict2.items() if k != \"model_list\"}\n",
    "\n",
    "    return dict1_filtered == dict2_filtered\n",
    "\n",
    "\n",
    "def config_name_provided(new_config, default_config_path, name):\n",
    "    with open(default_config_path, 'r') as f:\n",
    "        default_config = yaml.safe_load(f)\n",
    "        if not equal_configs(new_config, default_config) and not name:\n",
    "            return False    \n",
    "            \n",
    "    return True\n",
    "\n",
    "\n",
    "def update_config(new_config, config_file, new_model):\n",
    "    if os.path.isfile(config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        assert equal_configs(config, new_config)\n",
    "\n",
    "        if new_model and (new_model not in config[\"model_list\"]):\n",
    "            config[\"model_list\"].append(new_model)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(config_file), exist_ok=True)\n",
    "        with open(config_file, \"w\") as f:\n",
    "            yaml.dump(new_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if LLM_MODEL:\n",
    "    assert isinstance(LLM_MODEL, str)\n",
    "    assert LLM_MODEL in api_config\n",
    "assert config_name_provided(new_config=gen_config, default_config_path=default_gen_config_path, name=GEN_ANSWER_CONFIG_NAME)\n",
    "gen_config_name = (GEN_ANSWER_CONFIG_NAME if GEN_ANSWER_CONFIG_NAME else \"default\")\n",
    "\n",
    "\n",
    "if OVERRIDE_GEN_PARAMS:\n",
    "    gen_config.update(OVERRIDE_GEN_PARAMS)\n",
    "\n",
    "if LLM_MODEL:\n",
    "    gen_config[\"model_list\"].append(LLM_MODEL)\n",
    "\n",
    "gen_config_file = os.path.join(data_path, gen_config[\"bench_name\"], \"model_answer\", gen_config_name, \"config\", \"gen_answer_config.yaml\")\n",
    "\n",
    "update_config(new_config=gen_config, config_file=gen_config_file, new_model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'name': 'config of answer generation for arena-hard-v0.1', 'bench_name': 'arena-hard-v0.1', 'temperature': 0.0, 'max_tokens': 2048, 'num_choices': 1, 'model_list': ['gpt-3.5-turbo-0125', 'gpt-4-1106-preview', 'gpt-4o-mini', 'gpt-3.5-turbo-1106']}\n",
      "\n",
      "Output to data/arena-hard-v0.1/model_answer/default/gpt-3.5-turbo-0125.jsonl\n",
      "500 number of existing answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/arena-hard-v0.1/model_answer/default/gpt-4-1106-preview.jsonl\n",
      "500 number of existing answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/arena-hard-v0.1/model_answer/default/gpt-4o-mini.jsonl\n",
      "500 number of existing answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/arena-hard-v0.1/model_answer/default/gpt-3.5-turbo-1106.jsonl\n",
      "500 number of existing answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "generation_path = os.path.join(data_path, gen_config[\"bench_name\"], \"model_answer\", gen_config_name)\n",
    "generator = AnswerGenerator(generation_path, api_config_path)\n",
    "print(generator)\n",
    "generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LLM_JUDGE:\n",
    "    assert isinstance(LLM_JUDGE, str)\n",
    "    assert LLM_JUDGE in api_config\n",
    "assert config_name_provided(new_config=judge_config, default_config_path=default_judge_config_path, name=JUDGE_CONFIG_NAME)\n",
    "assert \"judge_model\" in judge_config\n",
    "\n",
    "if OVERRIDE_JUDGE_PARAMS:\n",
    "    judge_config.update(OVERRIDE_JUDGE_PARAMS)\n",
    "\n",
    "if LLM_JUDGE:\n",
    "    judge_config[\"judge_model\"] = LLM_JUDGE\n",
    "\n",
    "judge_config_name = judge_config[\"judge_model\"] + '_' + (JUDGE_CONFIG_NAME if JUDGE_CONFIG_NAME else \"default\")\n",
    "\n",
    "if LLM_MODEL:\n",
    "    judge_config[\"model_list\"].append(LLM_MODEL)\n",
    "\n",
    "judge_config_file = os.path.join(data_path, judge_config[\"bench_name\"], \"model_judgment\", judge_config_name, \"config\", \"judge_config.yaml\")\n",
    "\n",
    "update_config(new_config=judge_config, config_file=judge_config_file, new_model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "judge model: gpt-4-1106-preview, baseline: True, baseline model: gpt-3.5-turbo-0125,reference: False, reference models: None, temperature: 0, max tokens: 4096, pairwise: True\n",
      "\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n",
      "500 number of existing judgments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "judgment_path = os.path.join(data_path, judge_config[\"bench_name\"], \"model_judgment\", judge_config_name)\n",
    "judge = Judge(judgment_path, generation_path, api_config_path)\n",
    "print(judge)\n",
    "judge.judge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bench_name': 'arena-hard-v0.1', 'judge_name': 'gpt-4-1106-preview', 'baseline': 'gpt-3.5-turbo-0125', 'load_battles': False, 'load_bootstrap': False, 'show_elo': False, 'length_control': False, 'weight': 3, 'num_rounds': 100, 'output': False, 'first_game_only': False}\n",
      "\n",
      "Turning judgment results into battles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:21<00:00,  1.67s/it]\n",
      "bootstrap: 100%|██████████| 100/100 [00:06<00:00, 16.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Model                                    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Score </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      95% CI </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Avg. #Tokens </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ gpt-4-1106-preview                       │  90.9 │ (-1.0, 1.3) │          541 │\n",
       "│ gpt-4o-mini                              │  83.9 │ (-1.9, 1.5) │          448 │\n",
       "│ T-Tech-T-pro-it-1.0                      │  83.8 │ (-1.7, 1.5) │          502 │\n",
       "│ gigachat_max_26.20_uncen                 │  82.7 │ (-1.4, 1.7) │          514 │\n",
       "│ gigachat_max_with_censor                 │  80.0 │ (-1.8, 1.7) │          515 │\n",
       "│ vikhr-nemo-12b-instruct-r-21-09-24       │  79.8 │ (-1.7, 1.8) │          627 │\n",
       "│ gemma-2-9b-it-sppo-iter3                 │  73.6 │ (-1.8, 1.7) │          509 │\n",
       "│ T-Tech-T-lite-it-1.0                     │  71.0 │ (-2.2, 1.9) │          544 │\n",
       "│ qwen2.5-14b-instruct                     │  70.5 │ (-2.5, 2.2) │          434 │\n",
       "│ gigachat_pro_26.20_uncen                 │  70.4 │ (-1.6, 2.0) │          549 │\n",
       "│ gemma-2-9b-it                            │  69.2 │ (-2.1, 1.9) │          459 │\n",
       "│ CohereForAI/aya-expanse-8b               │  67.1 │ (-1.7, 1.9) │          698 │\n",
       "│ t-lite-instruct-0.1                      │  64.7 │ (-2.6, 2.6) │          810 │\n",
       "│ vikhr-llama3.1-8b-instruct-r-21-09-24    │  63.4 │ (-2.0, 2.3) │          618 │\n",
       "│ suzume-llama-3-8B-multilingual-orpo-bor… │  57.1 │ (-2.2, 1.6) │          682 │\n",
       "│ gigachat_lite_26.20_uncen                │  56.4 │ (-2.2, 2.5) │          561 │\n",
       "│ phi-3-medium-4k-instruct                 │  55.1 │ (-2.1, 1.9) │          566 │\n",
       "│ mistral-nemo-instruct-2407               │  50.5 │ (-2.8, 2.4) │          403 │\n",
       "│ yandex_gpt_pro_v4_26102024               │  50.5 │ (-2.5, 2.0) │          384 │\n",
       "│ sfr-iterative-dpo-llama-3-8b-r           │  50.1 │ (-2.4, 2.0) │          516 │\n",
       "│ gpt-3.5-turbo-0125                       │  50.0 │  (0.0, 0.0) │          220 │\n",
       "│ glm-4-9b-chat                            │  49.8 │ (-2.1, 2.3) │          568 │\n",
       "│ c4ai-command-r-v01                       │  49.0 │ (-2.6, 1.9) │          529 │\n",
       "│ llama-3-instruct-8b-sppo-iter3           │  47.5 │ (-2.4, 2.0) │          502 │\n",
       "│ suzume-llama-3-8b-multilingual           │  45.7 │ (-2.0, 2.2) │          641 │\n",
       "│ yandex_gpt_pro                           │  45.1 │ (-2.5, 1.9) │          345 │\n",
       "│ hermes-2-theta-llama-3-8b                │  44.1 │ (-2.0, 2.3) │          485 │\n",
       "│ yandex_gpt_lite_v4_26102024              │  42.7 │ (-2.0, 2.2) │          328 │\n",
       "│ gpt-3.5-turbo-1106                       │  41.5 │ (-1.6, 2.4) │          191 │\n",
       "│ llama-3-smaug-8b                         │  40.8 │ (-2.2, 2.0) │          524 │\n",
       "│ llama-3-8b-saiga-suzume-ties             │  39.9 │ (-2.2, 2.7) │          763 │\n",
       "│ starling-lm-7b-beta                      │  39.8 │ (-2.3, 2.4) │          629 │\n",
       "│ vikhr-it-5.4-fp16-orpo-v2                │  39.3 │ (-1.7, 2.1) │          379 │\n",
       "│ saiga_llama3_8b_v6                       │  39.2 │ (-2.2, 2.3) │          471 │\n",
       "│ llama-3-instruct-8b-simpo                │  38.0 │ (-2.2, 1.8) │          417 │\n",
       "│ qwen2-7b-instruct                        │  37.5 │ (-2.3, 2.2) │          340 │\n",
       "│ paralex-llama-3-8b-sft                   │  37.4 │ (-2.1, 2.4) │          688 │\n",
       "│ aya-23-8b                                │  36.3 │ (-2.1, 2.2) │          554 │\n",
       "│ meta-llama-3-8b-instruct                 │  35.1 │ (-2.2, 1.8) │          450 │\n",
       "│ openchat-3.5-0106                        │  33.8 │ (-2.1, 1.8) │          492 │\n",
       "│ mistral-7b-instruct-v0.3                 │  32.9 │ (-1.7, 1.9) │          469 │\n",
       "│ vikhr-it-5.2-fp16-cp                     │  31.7 │ (-1.5, 1.9) │          543 │\n",
       "│ hermes-2-pro-llama-3-8b                  │  30.8 │ (-2.4, 2.2) │          463 │\n",
       "│ openchat-3.6-8b-20240522                 │  30.3 │ (-2.5, 2.0) │          428 │\n",
       "│ vikhr-it-5.3-fp16-32k                    │  27.8 │ (-1.9, 2.2) │          519 │\n",
       "│ vikhr-it-5.3-fp16                        │  22.7 │ (-1.6, 1.7) │          523 │\n",
       "│ snorkel-mistral-pairrm-dpo               │  22.4 │ (-2.1, 1.7) │          773 │\n",
       "│ kolibri-vikhr-mistral-0427               │  22.4 │ (-1.8, 1.7) │          489 │\n",
       "│ storm-7b                                 │  20.6 │ (-1.6, 1.6) │          419 │\n",
       "│ neural-chat-7b-v3-3                      │  19.0 │ (-1.5, 1.6) │          927 │\n",
       "└──────────────────────────────────────────┴───────┴─────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mModel                                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mScore\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m     95% CI\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mAvg. #Tokens\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ gpt-4-1106-preview                       │  90.9 │ (-1.0, 1.3) │          541 │\n",
       "│ gpt-4o-mini                              │  83.9 │ (-1.9, 1.5) │          448 │\n",
       "│ T-Tech-T-pro-it-1.0                      │  83.8 │ (-1.7, 1.5) │          502 │\n",
       "│ gigachat_max_26.20_uncen                 │  82.7 │ (-1.4, 1.7) │          514 │\n",
       "│ gigachat_max_with_censor                 │  80.0 │ (-1.8, 1.7) │          515 │\n",
       "│ vikhr-nemo-12b-instruct-r-21-09-24       │  79.8 │ (-1.7, 1.8) │          627 │\n",
       "│ gemma-2-9b-it-sppo-iter3                 │  73.6 │ (-1.8, 1.7) │          509 │\n",
       "│ T-Tech-T-lite-it-1.0                     │  71.0 │ (-2.2, 1.9) │          544 │\n",
       "│ qwen2.5-14b-instruct                     │  70.5 │ (-2.5, 2.2) │          434 │\n",
       "│ gigachat_pro_26.20_uncen                 │  70.4 │ (-1.6, 2.0) │          549 │\n",
       "│ gemma-2-9b-it                            │  69.2 │ (-2.1, 1.9) │          459 │\n",
       "│ CohereForAI/aya-expanse-8b               │  67.1 │ (-1.7, 1.9) │          698 │\n",
       "│ t-lite-instruct-0.1                      │  64.7 │ (-2.6, 2.6) │          810 │\n",
       "│ vikhr-llama3.1-8b-instruct-r-21-09-24    │  63.4 │ (-2.0, 2.3) │          618 │\n",
       "│ suzume-llama-3-8B-multilingual-orpo-bor… │  57.1 │ (-2.2, 1.6) │          682 │\n",
       "│ gigachat_lite_26.20_uncen                │  56.4 │ (-2.2, 2.5) │          561 │\n",
       "│ phi-3-medium-4k-instruct                 │  55.1 │ (-2.1, 1.9) │          566 │\n",
       "│ mistral-nemo-instruct-2407               │  50.5 │ (-2.8, 2.4) │          403 │\n",
       "│ yandex_gpt_pro_v4_26102024               │  50.5 │ (-2.5, 2.0) │          384 │\n",
       "│ sfr-iterative-dpo-llama-3-8b-r           │  50.1 │ (-2.4, 2.0) │          516 │\n",
       "│ gpt-3.5-turbo-0125                       │  50.0 │  (0.0, 0.0) │          220 │\n",
       "│ glm-4-9b-chat                            │  49.8 │ (-2.1, 2.3) │          568 │\n",
       "│ c4ai-command-r-v01                       │  49.0 │ (-2.6, 1.9) │          529 │\n",
       "│ llama-3-instruct-8b-sppo-iter3           │  47.5 │ (-2.4, 2.0) │          502 │\n",
       "│ suzume-llama-3-8b-multilingual           │  45.7 │ (-2.0, 2.2) │          641 │\n",
       "│ yandex_gpt_pro                           │  45.1 │ (-2.5, 1.9) │          345 │\n",
       "│ hermes-2-theta-llama-3-8b                │  44.1 │ (-2.0, 2.3) │          485 │\n",
       "│ yandex_gpt_lite_v4_26102024              │  42.7 │ (-2.0, 2.2) │          328 │\n",
       "│ gpt-3.5-turbo-1106                       │  41.5 │ (-1.6, 2.4) │          191 │\n",
       "│ llama-3-smaug-8b                         │  40.8 │ (-2.2, 2.0) │          524 │\n",
       "│ llama-3-8b-saiga-suzume-ties             │  39.9 │ (-2.2, 2.7) │          763 │\n",
       "│ starling-lm-7b-beta                      │  39.8 │ (-2.3, 2.4) │          629 │\n",
       "│ vikhr-it-5.4-fp16-orpo-v2                │  39.3 │ (-1.7, 2.1) │          379 │\n",
       "│ saiga_llama3_8b_v6                       │  39.2 │ (-2.2, 2.3) │          471 │\n",
       "│ llama-3-instruct-8b-simpo                │  38.0 │ (-2.2, 1.8) │          417 │\n",
       "│ qwen2-7b-instruct                        │  37.5 │ (-2.3, 2.2) │          340 │\n",
       "│ paralex-llama-3-8b-sft                   │  37.4 │ (-2.1, 2.4) │          688 │\n",
       "│ aya-23-8b                                │  36.3 │ (-2.1, 2.2) │          554 │\n",
       "│ meta-llama-3-8b-instruct                 │  35.1 │ (-2.2, 1.8) │          450 │\n",
       "│ openchat-3.5-0106                        │  33.8 │ (-2.1, 1.8) │          492 │\n",
       "│ mistral-7b-instruct-v0.3                 │  32.9 │ (-1.7, 1.9) │          469 │\n",
       "│ vikhr-it-5.2-fp16-cp                     │  31.7 │ (-1.5, 1.9) │          543 │\n",
       "│ hermes-2-pro-llama-3-8b                  │  30.8 │ (-2.4, 2.2) │          463 │\n",
       "│ openchat-3.6-8b-20240522                 │  30.3 │ (-2.5, 2.0) │          428 │\n",
       "│ vikhr-it-5.3-fp16-32k                    │  27.8 │ (-1.9, 2.2) │          519 │\n",
       "│ vikhr-it-5.3-fp16                        │  22.7 │ (-1.6, 1.7) │          523 │\n",
       "│ snorkel-mistral-pairrm-dpo               │  22.4 │ (-2.1, 1.7) │          773 │\n",
       "│ kolibri-vikhr-mistral-0427               │  22.4 │ (-1.8, 1.7) │          489 │\n",
       "│ storm-7b                                 │  20.6 │ (-1.6, 1.6) │          419 │\n",
       "│ neural-chat-7b-v3-3                      │  19.0 │ (-1.5, 1.6) │          927 │\n",
       "└──────────────────────────────────────────┴───────┴─────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = Result(generation_path, judgment_path)\n",
    "print(result)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
